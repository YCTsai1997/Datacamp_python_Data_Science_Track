{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc59939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d700ae63",
   "metadata": {},
   "source": [
    "## Exploring your working directory\n",
    "- This means that the IPython magic command ! ls will display the contents of your current directory. Your task is to use the IPython magic command ! ls to check out the contents of your current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8295d39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS_10_Introduction to Importing Data in Python.ipynb\r\n",
      "DS_3_Data_Manipulation_with_Pandas.ipynb\r\n",
      "DS_4_Joining Data_with_Pandas.ipynb\r\n",
      "DS_5_Introduction to data visualization with Matplotlib.ipynb\r\n",
      "DS_6_Introduction_to_Seaborn.ipynb\r\n",
      "DS_7_Python Data Science Toolbox (Part 1).ipynb\r\n",
      "DS_8_Python Data Science Toolbox (Part 2).ipynb\r\n",
      "DS_9_Intermediate Data Visualization with Seaborn.ipynb\r\n",
      "\u001b[34mData\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497c21de",
   "metadata": {},
   "source": [
    " - you learned how to use the IPython magic command ! ls to explore your current working directory. You can also do this natively in Python using the library os, which consists of miscellaneous operating system interfaces.\n",
    "\n",
    "- The first line of the following code imports the library os, the second line stores the name of the current directory in a string called wd and the third outputs the contents of the directory in a list to the shell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a583d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'DS_6_Introduction_to_Seaborn.ipynb',\n",
       " 'Untitled.ipynb',\n",
       " 'DS_10_Introduction to Importing Data in Python.ipynb',\n",
       " 'DS_3_Data_Manipulation_with_Pandas.ipynb',\n",
       " 'DS_8_Python Data Science Toolbox (Part 2).ipynb',\n",
       " 'DS_7_Python Data Science Toolbox (Part 1).ipynb',\n",
       " 'DS_9_Intermediate Data Visualization with Seaborn.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'Data',\n",
       " 'DS_5_Introduction to data visualization with Matplotlib.ipynb',\n",
       " 'DS_4_Joining Data_with_Pandas.ipynb']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "wd = os.getcwd()\n",
    "os.listdir(wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a56c3a",
   "metadata": {},
   "source": [
    "## Reading a text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4583340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename= 'huck-finn.txt'\n",
    "file = open(filename, mode = 'r') # 'r is to read'\n",
    "text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594f454f",
   "metadata": {},
   "source": [
    "## Write to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1223dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename= 'huck-finn.txt'\n",
    "file = open(filename, mode = 'w') # 'r is to read'\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c4408e",
   "metadata": {},
   "source": [
    "## Importing text files line by line (Context manager with\n",
    "- For large files, we may not want to print all of their content to the shell: you may wish to print only the first few lines.\n",
    "- Enter the readline() method, which allows you to do this.\n",
    "- When a file called file is open, you can print out the first line by executing file.readline(). If you execute the same command again, the second line will print, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd4a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('huck-finn.txt', 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae1c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('huck-finn.txt', 'r') as file:\n",
    "    print(file.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a5b7e",
   "metadata": {},
   "source": [
    "## Importing entire text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af818729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file: file\n",
    "file = open('moby_dick.txt', 'r')\n",
    "\n",
    "# Print it\n",
    "print(file.read())\n",
    "\n",
    "# Check whether file is closed\n",
    "print(file.closed)\n",
    "\n",
    "# Close file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b489b850",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "410f780d",
   "metadata": {},
   "source": [
    "## NumPy \n",
    "- NUmPy arrays: standard for storing numerical data\n",
    "- Essential for other packages: e.g. scikit-learn\n",
    "- loadtxt()\n",
    "- genfromtxt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af1758b",
   "metadata": {},
   "source": [
    "## Import falt files using NumPy\n",
    "- There are a number of arguments that np.loadtxt() takes that you'll find useful:\n",
    "    - delimiter changes the delimiter that loadtxt() is expecting.\n",
    "        - You can use ',' for comma-delimited.\n",
    "        - You can use '\\t' for tab-delimited.\n",
    "    - skiprows allows you to specify how many rows (not indices) you wish to skip\n",
    "    - usecols takes a list of the indices of the columns you wish to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc9a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'MNIST.txt'\n",
    "data = np.loadtxt(filename, delimiter= ',', skiprows = 1, usecols= [0, 2])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd78c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(filename, delimiter = ',', dtype =str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449e47c",
   "metadata": {},
   "source": [
    "## Working with mixed datatypes\n",
    "- Much of the time you will need to import datasets which have different datatypes in different columns; one column may contain strings and another floats, for example. The function np.loadtxt() will freak at this. There is another function, np.genfromtxt(), which can handle such structures. If we pass dtype=None to it, it will figure out what types each column should be.\n",
    "- data = np.genfromtxt('titanic.csv', delimiter=',', names=True, dtype=None)\n",
    "    - the first argument is the filename\n",
    "    - the second specifies the delimiter \n",
    "    - the third argument names tells us there is a header. \n",
    "    - Because the data are of different types, data is an object called a structured array. Because numpy arrays have to contain elements that are all the same type, the structured array solves this by being a 1D array, where each element of the array is a row of the flat file imported. You can test this by checking out the array's shape in the shell by executing np.shape(data).\n",
    "- You have just used np.genfromtxt() to import data containing mixed datatypes. There is also another function np.recfromcsv() that behaves similarly to np.genfromtxt(), except that its default dtype is None. (defaults delimiter=',' and names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66794ea",
   "metadata": {},
   "source": [
    "## Using pandas to import flat files as DataFrames\n",
    "- missing values are also commonly referred to as NA or NaN\n",
    "- contains comments after the character '#'\n",
    "- is tab-delimited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd97d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the filename: file\n",
    "file = 'digits.csv'\n",
    "\n",
    "# Read the first 5 rows of the file into a DataFrame: data\n",
    "data = pd.read_csv(file, nrows = 5, header = None)\n",
    "\n",
    "# Build a numpy array from the DataFrame: data_array\n",
    "data_array = np.array(data)\n",
    "\n",
    "# Print the datatype of data_array to the shell\n",
    "print(type(data_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051e31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign filename: file\n",
    "file = 'titanic_corrupt.txt'\n",
    "\n",
    "# Import file: data\n",
    "data = pd.read_csv(file, sep='\\t', comment='#', na_values=['Nothing'])\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d113c59",
   "metadata": {},
   "source": [
    "## Pickled files\n",
    "- File type native to Python\n",
    "- Motivation: many datatypes for which it isn;t obvious how to store them \n",
    "- Pickled files are serialized\n",
    "- Serialized. = convert object to bytestream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pickled_fruit.pkl','rb') as file: #one signifying 'read only', the other 'binary'\n",
    "    data = pickle.load(file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f493e4e",
   "metadata": {},
   "source": [
    "## Importing Excel speardsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d825b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'urbanpop.xlsx'\n",
    "data = pd.ExcelFile(file)\n",
    "print(data.sheet_names)\n",
    "\n",
    "df1 = data.parse('1960-1966') # Sheet name, as string\n",
    "df2 = data.parse(0) # Sheet index, as a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dd44c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The spreadsheet 'battledeath.xlsx' is already loaded as xls.\n",
    "Parse the first sheet by index. \n",
    "In doing so, skip the first row of data and name the columns 'Country' and 'AAM due to War (2002)' \n",
    "    using the argument names. The values passed to skiprows and names all need to be of type list.\n",
    "\"\"\"\n",
    "\n",
    "# Parse the first sheet and rename the columns: df1\n",
    "df1 = xls.parse(0, skiprows=1, names=['Country','AAM due to War (2002)'])\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f7c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The spreadsheet 'battledeath.xlsx' is already loaded as xls.\n",
    "Parse the second sheet by index. \n",
    "In doing so, parse only the first column with the usecols parameter, \n",
    "    skip the first row and rename the column 'Country'. \n",
    "The argument passed to usecols also needs to be of type list.\n",
    "\"\"\"\n",
    "\n",
    "# Parse the first column of the second sheet and rename the column: df2\n",
    "df2 = xls.parse(1, usecols=[0], skiprows=[0], names=['Country'])\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2bd31e",
   "metadata": {},
   "source": [
    "## Importing SAS/State files using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d47e134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sas7bdat import SAS7BDAT\n",
    "with SAS7BDAT('urbanpop.sas7bdat') as file:\n",
    "    df_sas = file.to_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff61ac4",
   "metadata": {},
   "source": [
    "## Importing Stata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b127e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_stata('urbanpop.dta')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6375138",
   "metadata": {},
   "source": [
    "## Importing HDF5 flies\n",
    "- Hierarchical Data Format version 5\n",
    "- Standard for storing large quantities of numerical data\n",
    "- Satasets can be hundreds of gigabytes or terabytes\n",
    "- HDF5 can scale to exabytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74991d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "filename = 'H-H1_LOSC_4_V1.hdf5'\n",
    "data = h5py.File(filename,'r') #'r' is to read\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b590e6a4",
   "metadata": {},
   "source": [
    "## The structure of HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35852a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data.keys():\n",
    "    print(key) #results is an HDF group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e3f08",
   "metadata": {},
   "source": [
    "## importing MATLAB files\n",
    " - keys = matlab variable names\n",
    " - values = objects assigned to variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "filename = 'workspace.mat'\n",
    "mat =scipy.io.loadmat(filename)\n",
    "print(type(mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060fef8d",
   "metadata": {},
   "source": [
    "## Creating a database engine\n",
    "- SQLite database: Fast and simple\n",
    "- SQLAlchemy: Works with many relational database management systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220501c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlit:///Northwind.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2e651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = engine.table_names()\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8767599f",
   "metadata": {},
   "source": [
    "## Querying relational databased in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da206664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "engine = create_engine('sqlit:///Northwind.sqlite')\n",
    "con = engine.connect()\n",
    "rs = con.execute(\"Select * from Orders\")\n",
    "df = pd.DataFrame(rs.fetchall())\n",
    "df.columns = rs.keys()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1bba22",
   "metadata": {},
   "source": [
    "## Using the context manager "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "engine = create_engine('sqlit:///Northwind.sqlite')\n",
    "\n",
    "with engine.connect() as con: \n",
    "    rs = con.execute(\"Select order_id, OrderDate, ShipName from Orders\")\n",
    "    df = pd.DataFrame(rs.fetchmany(size=5))\n",
    "    df.columns = rs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c05e4c",
   "metadata": {},
   "source": [
    "## The pands way to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d189758",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"Select * from Orders\", engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec18745d",
   "metadata": {},
   "source": [
    "## Advanced querying: Exploiting table relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea736542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query(\" select * from PlaylistTrack INNER JOIN Track on PlaylistTrack.TrackId = Track.TrackId where Milliseconds < 250000\", engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
